{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a148afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ python gridworld_qlearning.py\n",
      "Initializing Gridworld...\n",
      "Grid size: 5x5\n",
      "Special_states = {'A': (0, 1), 'B': (0, 3)}\n",
      "Next_to_states = {\"A'\": (4, 1), \"B'\": (2, 3)}\n",
      "Special_rewards = {'A': 10, 'B': 5}\n",
      "Starting Q-learning with parameters:\n",
      "  γ = 0.9\n",
      "  ε = 0.1\n",
      "  α = 0.2\n",
      "  Episodes = 5000\n",
      "  Steps = 5000\n",
      "\n",
      "Evaluating optimal value function and policy...\n",
      "Optimal Value Function:\n",
      "21.98  24.42  21.98  19.42  17.48\n",
      "19.78  21.98  19.78  17.80  16.02\n",
      "17.80  19.78  17.80  16.02  14.42\n",
      "16.02  17.80  16.02  14.42  12.98\n",
      "14.42  16.02  14.42  12.98  11.68\n",
      "\n",
      "Optimal Policy:\n",
      "east    north   west    north   west  \n",
      "north   north   north   west    west  \n",
      "north   north   north   north   north \n",
      "north   north   north   north   north \n",
      "north   north   north   north   north \n",
      "\n",
      "Optimal Policy (arrows):\n",
      "  →       ↑       ←       ↑       ←   \n",
      "  ↑       ↑       ↑       ←       ←   \n",
      "  ↑       ↑       ↑       ↑       ↑   \n",
      "  ↑       ↑       ↑       ↑       ↑   \n",
      "  ↑       ↑       ↑       ↑       ↑   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld setup\n",
    "grid_size = (5, 5)\n",
    "special_states = {'A': (0, 1), 'B': (0, 3)}\n",
    "next_to_states = {\"A'\": (4, 1), \"B'\": (2, 3)}\n",
    "special_rewards = {'A': 10, 'B': 5}\n",
    "default_reward = 0\n",
    "edge_penalty = -1\n",
    "\n",
    "# Actions: north, south, east, west\n",
    "actions = ['north', 'south', 'east', 'west']\n",
    "action_arrows = {'north': '↑', 'south': '↓', 'east': '→', 'west': '←'}\n",
    "\n",
    "# Q-learning parameters\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "alpha = 0.2  # Learning rate\n",
    "episodes = 5000\n",
    "steps_per_episode = 5000\n",
    "\n",
    "# Initialize Q-values\n",
    "Q = np.zeros((grid_size[0], grid_size[1], len(actions)))\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    row, col = state\n",
    "    if action == 'north':\n",
    "        next_row, next_col = row - 1, col\n",
    "    elif action == 'south':\n",
    "        next_row, next_col = row + 1, col\n",
    "    elif action == 'east':\n",
    "        next_row, next_col = row, col + 1\n",
    "    elif action == 'west':\n",
    "        next_row, next_col = row, col - 1\n",
    "    \n",
    "    # Check if next state is valid\n",
    "    if (0 <= next_row < grid_size[0]) and (0 <= next_col < grid_size[1]):\n",
    "        return (next_row, next_col)\n",
    "    else:\n",
    "        return state  # Stay in place if off-grid\n",
    "\n",
    "def get_reward(state, action):\n",
    "    # Check for special states\n",
    "    for special_state, pos in special_states.items():\n",
    "        if state == pos:\n",
    "            return special_rewards[special_state]\n",
    "    \n",
    "    # Check if action leads off-grid\n",
    "    next_state = get_next_state(state, action)\n",
    "    if next_state == state:  # Hit edge\n",
    "        return edge_penalty\n",
    "    else:\n",
    "        return default_reward\n",
    "\n",
    "def get_teleport_state(state):\n",
    "    # Teleport from A to A' or B to B'\n",
    "    for special_state, pos in special_states.items():\n",
    "        if state == pos:\n",
    "            return next_to_states.get(f\"{special_state}'\", state)\n",
    "    return state\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = (np.random.randint(grid_size[0]), np.random.randint(grid_size[1]))  # Random start\n",
    "    for step in range(steps_per_episode):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            action_idx = np.argmax(Q[state[0], state[1], :])\n",
    "            action = actions[action_idx]\n",
    "        \n",
    "        # Execute action\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = get_reward(state, action)\n",
    "        \n",
    "        # Handle teleportation for special states\n",
    "        if state in special_states.values():\n",
    "            next_state = get_teleport_state(state)\n",
    "        \n",
    "        # Q-value update\n",
    "        best_next_action = np.argmax(Q[next_state[0], next_state[1], :])\n",
    "        td_target = reward + gamma * Q[next_state[0], next_state[1], best_next_action]\n",
    "        td_error = td_target - Q[state[0], state[1], actions.index(action)]\n",
    "        Q[state[0], state[1], actions.index(action)] += alpha * td_error\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "# Extract optimal value function and policy\n",
    "optimal_value_function = np.max(Q, axis=2)\n",
    "optimal_policy_indices = np.argmax(Q, axis=2)\n",
    "optimal_policy = [[actions[idx] for idx in row] for row in optimal_policy_indices]\n",
    "optimal_policy_arrows = [[action_arrows[action] for action in row] for row in optimal_policy]\n",
    "\n",
    "# Print results\n",
    "print(\"$ python gridworld_qlearning.py\")\n",
    "print(\"Initializing Gridworld...\")\n",
    "print(f\"Grid size: {grid_size[0]}x{grid_size[1]}\")\n",
    "print(f\"Special_states = {special_states}\")\n",
    "print(f\"Next_to_states = {next_to_states}\")\n",
    "print(f\"Special_rewards = {special_rewards}\")\n",
    "print(\"Starting Q-learning with parameters:\")\n",
    "print(f\"  γ = {gamma}\")\n",
    "print(f\"  ε = {epsilon}\")\n",
    "print(f\"  α = {alpha}\")\n",
    "print(f\"  Episodes = {episodes}\")\n",
    "print(f\"  Steps = {steps_per_episode}\\n\")\n",
    "print(\"Evaluating optimal value function and policy...\")\n",
    "print(\"Optimal Value Function:\")\n",
    "for row in optimal_value_function:\n",
    "    print(\"  \".join(f\"{val:5.2f}\" for val in row))\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for row in optimal_policy:\n",
    "    print(\"  \".join(f\"{action:<6}\" for action in row))\n",
    "print(\"\\nOptimal Policy (arrows):\")\n",
    "for row in optimal_policy_arrows:\n",
    "    print(\"  \".join(f\"{arrow:^6}\" for arrow in row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
